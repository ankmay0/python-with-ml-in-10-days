{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Day 7: Classification Mastery\n",
                "\n",
                "Today we tackle **Classification** - predicting categories instead of numbers!\n",
                "\n",
                "### Topics Covered:\n",
                "1. Classification vs Regression\n",
                "2. Logistic Regression\n",
                "3. Decision Trees\n",
                "4. Random Forests\n",
                "5. Model Evaluation (Accuracy, Precision, Recall, F1)\n",
                "6. **Mini Project: Titanic Survival Prediction**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
                "from sklearn.metrics import confusion_matrix, classification_report\n",
                "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
                "\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "np.random.seed(42)\n",
                "print(\"Libraries loaded!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Classification vs Regression\n",
                "\n",
                "| Aspect | Regression | Classification |\n",
                "|--------|------------|----------------|\n",
                "| **Output** | Continuous (price, temp) | Discrete (yes/no, A/B/C) |\n",
                "| **Example** | Predict house price | Spam or not spam |\n",
                "| **Algorithms** | Linear Regression | Logistic Regression, Trees |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Binary vs Multiclass Classification\n",
                "print(\"CLASSIFICATION TYPES\")\n",
                "print(\"=\"*50)\n",
                "print(\"\\nBINARY: 2 classes\")\n",
                "print(\"  • Spam / Not Spam\")\n",
                "print(\"  • Survived / Died\")\n",
                "print(\"  • Fraud / Legitimate\")\n",
                "print(\"\\nMULTICLASS: 3+ classes\")\n",
                "print(\"  • Cat / Dog / Bird\")\n",
                "print(\"  • Low / Medium / High\")\n",
                "print(\"  • Digits 0-9\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Logistic Regression\n",
                "\n",
                "Despite the name, it's a **classification** algorithm! Uses sigmoid function to output probabilities (0-1).\n",
                "\n",
                "```\n",
                "P(y=1) = 1 / (1 + e^(-z))  where z = wx + b\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize the Sigmoid Function\n",
                "z = np.linspace(-10, 10, 100)\n",
                "sigmoid = 1 / (1 + np.exp(-z))\n",
                "\n",
                "plt.figure(figsize=(10, 5))\n",
                "plt.plot(z, sigmoid, 'b-', linewidth=3)\n",
                "plt.axhline(y=0.5, color='r', linestyle='--', label='Decision Boundary (0.5)')\n",
                "plt.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
                "plt.fill_between(z, sigmoid, 0.5, where=(sigmoid > 0.5), alpha=0.3, color='green', label='Predict Class 1')\n",
                "plt.fill_between(z, sigmoid, 0.5, where=(sigmoid < 0.5), alpha=0.3, color='red', label='Predict Class 0')\n",
                "plt.title('Sigmoid Function - Heart of Logistic Regression', fontsize=14, fontweight='bold')\n",
                "plt.xlabel('z = wx + b')\n",
                "plt.ylabel('Probability P(y=1)')\n",
                "plt.legend()\n",
                "plt.ylim(-0.1, 1.1)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Simple Logistic Regression Example\n",
                "np.random.seed(42)\n",
                "# Study hours -> Pass/Fail\n",
                "hours = np.concatenate([np.random.normal(3, 1, 50), np.random.normal(7, 1, 50)])\n",
                "passed = np.array([0]*50 + [1]*50)\n",
                "\n",
                "X = hours.reshape(-1, 1)\n",
                "y = passed\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                "\n",
                "# Train model\n",
                "log_reg = LogisticRegression()\n",
                "log_reg.fit(X_train, y_train)\n",
                "\n",
                "# Predictions\n",
                "y_pred = log_reg.predict(X_test)\n",
                "accuracy = accuracy_score(y_test, y_pred)\n",
                "\n",
                "print(f\"Accuracy: {accuracy*100:.1f}%\")\n",
                "print(f\"\\nCoefficient: {log_reg.coef_[0][0]:.3f}\")\n",
                "print(f\"Intercept: {log_reg.intercept_[0]:.3f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Decision Trees\n",
                "\n",
                "Makes decisions by asking a series of questions, creating a tree-like structure."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create sample dataset\n",
                "np.random.seed(42)\n",
                "n = 200\n",
                "age = np.random.randint(18, 70, n)\n",
                "income = np.random.randint(20000, 150000, n)\n",
                "# Buy if: (age > 30 AND income > 50000) OR income > 100000\n",
                "will_buy = ((age > 30) & (income > 50000)) | (income > 100000)\n",
                "will_buy = will_buy.astype(int)\n",
                "\n",
                "df = pd.DataFrame({'Age': age, 'Income': income, 'Will_Buy': will_buy})\n",
                "\n",
                "X = df[['Age', 'Income']]\n",
                "y = df['Will_Buy']\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                "\n",
                "# Train Decision Tree\n",
                "tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
                "tree.fit(X_train, y_train)\n",
                "\n",
                "print(f\"Decision Tree Accuracy: {tree.score(X_test, y_test)*100:.1f}%\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize the Decision Tree\n",
                "plt.figure(figsize=(15, 8))\n",
                "plot_tree(tree, feature_names=['Age', 'Income'], class_names=['No', 'Yes'],\n",
                "          filled=True, rounded=True, fontsize=10)\n",
                "plt.title('Decision Tree Visualization', fontsize=14, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Random Forests\n",
                "\n",
                "An **ensemble** of many decision trees. Each tree votes, and majority wins!\n",
                "\n",
                "**Advantages:**\n",
                "- Reduces overfitting\n",
                "- More robust predictions\n",
                "- Provides feature importance"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train Random Forest\n",
                "rf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
                "rf.fit(X_train, y_train)\n",
                "\n",
                "print(f\"Random Forest Accuracy: {rf.score(X_test, y_test)*100:.1f}%\")\n",
                "print(f\"\\nFeature Importances:\")\n",
                "for feat, imp in zip(X.columns, rf.feature_importances_):\n",
                "    print(f\"  {feat}: {imp:.3f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compare all 3 models\n",
                "models = {\n",
                "    'Logistic Regression': LogisticRegression(),\n",
                "    'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=42),\n",
                "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42)\n",
                "}\n",
                "\n",
                "print(\"MODEL COMPARISON\")\n",
                "print(\"=\"*40)\n",
                "for name, model in models.items():\n",
                "    model.fit(X_train, y_train)\n",
                "    acc = model.score(X_test, y_test)\n",
                "    print(f\"{name:25}: {acc*100:.1f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Evaluation Metrics\n",
                "\n",
                "| Metric | Formula | When to Use |\n",
                "|--------|---------|-------------|\n",
                "| **Accuracy** | Correct / Total | Balanced classes |\n",
                "| **Precision** | TP / (TP + FP) | Cost of false positives high |\n",
                "| **Recall** | TP / (TP + FN) | Cost of false negatives high |\n",
                "| **F1 Score** | 2 × (P×R)/(P+R) | Balance precision & recall |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Confusion Matrix Visualization\n",
                "y_pred_rf = rf.predict(X_test)\n",
                "cm = confusion_matrix(y_test, y_pred_rf)\n",
                "\n",
                "plt.figure(figsize=(8, 6))\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
                "            xticklabels=['No Buy', 'Buy'], yticklabels=['No Buy', 'Buy'])\n",
                "plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
                "plt.xlabel('Predicted')\n",
                "plt.ylabel('Actual')\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nClassification Report:\")\n",
                "print(classification_report(y_test, y_pred_rf, target_names=['No Buy', 'Buy']))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Mini Project: Titanic Survival Prediction\n",
                "\n",
                "**Goal:** Predict if a passenger survived the Titanic disaster."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create Titanic-like dataset\n",
                "np.random.seed(42)\n",
                "n = 800\n",
                "\n",
                "pclass = np.random.choice([1, 2, 3], n, p=[0.25, 0.25, 0.50])\n",
                "sex = np.random.choice(['male', 'female'], n, p=[0.65, 0.35])\n",
                "age = np.random.normal(30, 15, n).clip(1, 80)\n",
                "fare = np.where(pclass == 1, np.random.normal(80, 30, n),\n",
                "                np.where(pclass == 2, np.random.normal(30, 15, n),\n",
                "                         np.random.normal(15, 10, n))).clip(5, 200)\n",
                "sibsp = np.random.choice([0, 1, 2, 3], n, p=[0.6, 0.25, 0.1, 0.05])\n",
                "\n",
                "# Survival logic\n",
                "survival_prob = 0.3\n",
                "survival_prob += np.where(sex == 'female', 0.35, -0.1)\n",
                "survival_prob += np.where(pclass == 1, 0.25, np.where(pclass == 2, 0.1, -0.15))\n",
                "survival_prob += np.where(age < 18, 0.15, np.where(age > 60, -0.1, 0))\n",
                "survived = (np.random.random(n) < survival_prob).astype(int)\n",
                "\n",
                "titanic = pd.DataFrame({\n",
                "    'Pclass': pclass, 'Sex': sex, 'Age': age.round(0).astype(int),\n",
                "    'Fare': fare.round(2), 'SibSp': sibsp, 'Survived': survived\n",
                "})\n",
                "\n",
                "print(\" TITANIC DATASET\")\n",
                "print(f\"Total Passengers: {len(titanic)}\")\n",
                "print(f\"Survived: {titanic['Survived'].sum()} ({titanic['Survived'].mean()*100:.1f}%)\")\n",
                "print(titanic.head(10))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# EDA: Survival Rates\n",
                "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
                "\n",
                "# By Sex\n",
                "titanic.groupby('Sex')['Survived'].mean().plot(kind='bar', ax=axes[0], color=['#3498db', '#e74c3c'])\n",
                "axes[0].set_title('Survival by Sex')\n",
                "axes[0].set_ylabel('Survival Rate')\n",
                "axes[0].set_xticklabels(['Female', 'Male'], rotation=0)\n",
                "\n",
                "# By Class\n",
                "titanic.groupby('Pclass')['Survived'].mean().plot(kind='bar', ax=axes[1], color='#2ecc71')\n",
                "axes[1].set_title('Survival by Class')\n",
                "axes[1].set_ylabel('Survival Rate')\n",
                "axes[1].set_xticklabels(['1st', '2nd', '3rd'], rotation=0)\n",
                "\n",
                "# Age Distribution\n",
                "titanic[titanic['Survived']==1]['Age'].hist(alpha=0.7, label='Survived', ax=axes[2], color='#2ecc71')\n",
                "titanic[titanic['Survived']==0]['Age'].hist(alpha=0.7, label='Died', ax=axes[2], color='#e74c3c')\n",
                "axes[2].set_title('Age Distribution by Survival')\n",
                "axes[2].legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare Data\n",
                "titanic_ml = titanic.copy()\n",
                "titanic_ml['Sex'] = LabelEncoder().fit_transform(titanic_ml['Sex'])  # male=1, female=0\n",
                "\n",
                "X = titanic_ml[['Pclass', 'Sex', 'Age', 'Fare', 'SibSp']]\n",
                "y = titanic_ml['Survived']\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                "\n",
                "# Scale features\n",
                "scaler = StandardScaler()\n",
                "X_train_scaled = scaler.fit_transform(X_train)\n",
                "X_test_scaled = scaler.transform(X_test)\n",
                "\n",
                "print(f\"Training: {len(X_train)} | Test: {len(X_test)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train Multiple Models\n",
                "models = {\n",
                "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
                "    'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=42),\n",
                "    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
                "}\n",
                "\n",
                "results = []\n",
                "print(\" MODEL COMPARISON\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "for name, model in models.items():\n",
                "    if name == 'Logistic Regression':\n",
                "        model.fit(X_train_scaled, y_train)\n",
                "        y_pred = model.predict(X_test_scaled)\n",
                "    else:\n",
                "        model.fit(X_train, y_train)\n",
                "        y_pred = model.predict(X_test)\n",
                "    \n",
                "    acc = accuracy_score(y_test, y_pred)\n",
                "    prec = precision_score(y_test, y_pred)\n",
                "    rec = recall_score(y_test, y_pred)\n",
                "    f1 = f1_score(y_test, y_pred)\n",
                "    \n",
                "    results.append({'Model': name, 'Accuracy': acc, 'Precision': prec, 'Recall': rec, 'F1': f1})\n",
                "    print(f\"\\n{name}:\")\n",
                "    print(f\"  Accuracy:  {acc*100:.1f}%\")\n",
                "    print(f\"  Precision: {prec*100:.1f}%\")\n",
                "    print(f\"  Recall:    {rec*100:.1f}%\")\n",
                "    print(f\"  F1 Score:  {f1*100:.1f}%\")\n",
                "\n",
                "results_df = pd.DataFrame(results)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Best Model Analysis\n",
                "best_model = models['Random Forest']\n",
                "y_pred_best = best_model.predict(X_test)\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
                "\n",
                "# Confusion Matrix\n",
                "cm = confusion_matrix(y_test, y_pred_best)\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
                "            xticklabels=['Died', 'Survived'], yticklabels=['Died', 'Survived'])\n",
                "axes[0].set_title('Confusion Matrix - Random Forest', fontweight='bold')\n",
                "axes[0].set_xlabel('Predicted')\n",
                "axes[0].set_ylabel('Actual')\n",
                "\n",
                "# Feature Importance\n",
                "importance = pd.DataFrame({'Feature': X.columns, 'Importance': best_model.feature_importances_})\n",
                "importance = importance.sort_values('Importance', ascending=True)\n",
                "axes[1].barh(importance['Feature'], importance['Importance'], color=plt.cm.viridis(np.linspace(0.3, 0.9, 5)))\n",
                "axes[1].set_title('Feature Importance', fontweight='bold')\n",
                "axes[1].set_xlabel('Importance')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Predict New Passengers\n",
                "new_passengers = pd.DataFrame({\n",
                "    'Pclass': [1, 3, 2, 1, 3],\n",
                "    'Sex': [0, 1, 0, 1, 0],  # 0=female, 1=male\n",
                "    'Age': [25, 35, 8, 60, 22],\n",
                "    'Fare': [100, 10, 30, 150, 8],\n",
                "    'SibSp': [1, 0, 2, 0, 1]\n",
                "})\n",
                "\n",
                "predictions = best_model.predict(new_passengers)\n",
                "probabilities = best_model.predict_proba(new_passengers)[:, 1]\n",
                "\n",
                "print(\" NEW PASSENGER PREDICTIONS\")\n",
                "print(\"=\"*60)\n",
                "for i, (pred, prob) in enumerate(zip(predictions, probabilities)):\n",
                "    p = new_passengers.iloc[i]\n",
                "    sex = 'Female' if p['Sex'] == 0 else 'Male'\n",
                "    status = 'SURVIVED' if pred == 1 else 'DIED'\n",
                "    print(f\"\\nPassenger {i+1}: {sex}, Age {p['Age']}, Class {p['Pclass']}\")\n",
                "    print(f\"  Prediction: {status} (Probability: {prob*100:.1f}%)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Final Summary\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\" DAY 7 COMPLETE!\")\n",
                "print(\"=\"*60)\n",
                "print(\"\"\"\n",
                " KEY TAKEAWAYS:\n",
                "\n",
                " 1. LOGISTIC REGRESSION\n",
                "    - Uses sigmoid function for probabilities\n",
                "    - Good baseline, interpretable\n",
                "\n",
                " 2. DECISION TREES\n",
                "    - Easy to visualize and understand\n",
                "    - Prone to overfitting\n",
                "\n",
                " 3. RANDOM FORESTS\n",
                "    - Ensemble of trees, reduces overfitting\n",
                "    - Usually best performance\n",
                "\n",
                " 4. METRICS\n",
                "    - Accuracy: Overall correctness\n",
                "    - Precision: Quality of positive predictions\n",
                "    - Recall: Coverage of actual positives\n",
                "    - F1: Balance of precision and recall\n",
                "\"\"\")\n",
                "print(\"=\"*60)\n",
                "print(\" Next: Day 8 - Unsupervised Learning!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Practice Exercises\n",
                "\n",
                "1. Try different `max_depth` values for Decision Tree\n",
                "2. Experiment with `n_estimators` in Random Forest\n",
                "3. Add new features like `Parch` (parents/children aboard)\n",
                "4. Use `cross_val_score` for robust evaluation\n",
                "\n",
                "---\n",
                "**Next Up:** Day 8 - Unsupervised Learning (K-Means, PCA)!"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
        "language_info": {"name": "python", "version": "3.13.8"}
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
