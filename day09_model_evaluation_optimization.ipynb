{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Day 9: Model Evaluation & Optimization\n",
                "\n",
                "Today we learn how to **evaluate** models properly and **tune** them for better performance!\n",
                "\n",
                "### Topics Covered:\n",
                "1. Evaluation Metrics Deep Dive\n",
                "2. Confusion Matrix Analysis\n",
                "3. Cross-Validation\n",
                "4. Hyperparameter Tuning (GridSearchCV)\n",
                "5. **Mini Project: Optimize Titanic Model**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.tree import DecisionTreeClassifier\n",
                "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
                "                             confusion_matrix, classification_report, roc_curve, auc)\n",
                "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "np.random.seed(42)\n",
                "print(\"Libraries loaded!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Evaluation Metrics Deep Dive\n",
                "\n",
                "### The Confusion Matrix Components\n",
                "```\n",
                "                 Predicted\n",
                "              Negative  Positive\n",
                "Actual  Neg     TN        FP      ← False Positive (Type I Error)\n",
                "        Pos     FN        TP      ← False Negative (Type II Error)\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Understand metrics with a concrete example\n",
                "print(\" METRICS EXPLAINED\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Simulated predictions\n",
                "y_true = np.array([1, 1, 1, 1, 1, 0, 0, 0, 0, 0])  # 5 positive, 5 negative\n",
                "y_pred = np.array([1, 1, 1, 0, 0, 0, 0, 0, 1, 1])  # Model predictions\n",
                "\n",
                "# Calculate components\n",
                "TP = sum((y_true == 1) & (y_pred == 1))  # True Positive\n",
                "TN = sum((y_true == 0) & (y_pred == 0))  # True Negative\n",
                "FP = sum((y_true == 0) & (y_pred == 1))  # False Positive\n",
                "FN = sum((y_true == 1) & (y_pred == 0))  # False Negative\n",
                "\n",
                "print(f\"\\nConfusion Matrix Components:\")\n",
                "print(f\"  True Positives (TP):  {TP}\")\n",
                "print(f\"  True Negatives (TN):  {TN}\")\n",
                "print(f\"  False Positives (FP): {FP} (Type I Error)\")\n",
                "print(f\"  False Negatives (FN): {FN} (Type II Error)\")\n",
                "\n",
                "# Calculate metrics manually\n",
                "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
                "precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
                "recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
                "f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
                "\n",
                "print(f\"\\nCalculations:\")\n",
                "print(f\"  Accuracy  = (TP+TN)/(All) = ({TP}+{TN})/10 = {accuracy:.1%}\")\n",
                "print(f\"  Precision = TP/(TP+FP) = {TP}/({TP}+{FP}) = {precision:.1%}\")\n",
                "print(f\"  Recall    = TP/(TP+FN) = {TP}/({TP}+{FN}) = {recall:.1%}\")\n",
                "print(f\"  F1 Score  = 2*(P*R)/(P+R) = {f1:.1%}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# When to use which metric?\n",
                "print(\"\\n WHEN TO USE WHICH METRIC?\")\n",
                "print(\"=\"*60)\n",
                "print(\"\"\"\n",
                " ACCURACY: When classes are balanced\n",
                "   Example: 50% spam, 50% not spam\n",
                "\n",
                " PRECISION: When False Positives are costly\n",
                "   Example: Email marked as spam but it's important\n",
                "   Question: \"Of all predicted positives, how many are correct?\"\n",
                "\n",
                " RECALL: When False Negatives are costly  \n",
                "   Example: Cancer detection - missing a case is dangerous\n",
                "   Question: \"Of all actual positives, how many did we catch?\"\n",
                "\n",
                " F1 SCORE: When you need balance between Precision & Recall\n",
                "   Example: Most real-world problems\n",
                "\"\"\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Confusion Matrix Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create sample dataset\n",
                "np.random.seed(42)\n",
                "n = 500\n",
                "\n",
                "# Features\n",
                "X = np.random.randn(n, 4)\n",
                "# Target based on features with some noise\n",
                "y = (X[:, 0] + X[:, 1] * 0.5 + np.random.randn(n) * 0.5 > 0).astype(int)\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                "\n",
                "# Train model\n",
                "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
                "rf.fit(X_train, y_train)\n",
                "y_pred = rf.predict(X_test)\n",
                "\n",
                "# Confusion Matrix\n",
                "cm = confusion_matrix(y_test, y_pred)\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
                "\n",
                "# Raw counts\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
                "            xticklabels=['Neg', 'Pos'], yticklabels=['Neg', 'Pos'])\n",
                "axes[0].set_title('Confusion Matrix (Counts)', fontweight='bold')\n",
                "axes[0].set_xlabel('Predicted')\n",
                "axes[0].set_ylabel('Actual')\n",
                "\n",
                "# Normalized\n",
                "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
                "sns.heatmap(cm_norm, annot=True, fmt='.2%', cmap='Greens', ax=axes[1],\n",
                "            xticklabels=['Neg', 'Pos'], yticklabels=['Neg', 'Pos'])\n",
                "axes[1].set_title('Confusion Matrix (Normalized)', fontweight='bold')\n",
                "axes[1].set_xlabel('Predicted')\n",
                "axes[1].set_ylabel('Actual')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nClassification Report:\")\n",
                "print(classification_report(y_test, y_pred))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Cross-Validation\n",
                "\n",
                "**Problem:** Single train/test split can be lucky or unlucky.\n",
                "\n",
                "**Solution:** K-Fold Cross-Validation - split data K ways, train K times, average results."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize K-Fold\n",
                "print(\" K-FOLD CROSS-VALIDATION\")\n",
                "print(\"=\"*50)\n",
                "print(\"\"\"\n",
                "5-Fold Example:\n",
                "\n",
                "Fold 1: [TEST] [Train] [Train] [Train] [Train]\n",
                "Fold 2: [Train] [TEST] [Train] [Train] [Train]\n",
                "Fold 3: [Train] [Train] [TEST] [Train] [Train]\n",
                "Fold 4: [Train] [Train] [Train] [TEST] [Train]\n",
                "Fold 5: [Train] [Train] [Train] [Train] [TEST]\n",
                "\n",
                "Final Score = Average of all 5 folds\n",
                "\"\"\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cross-validation in practice\n",
                "models = {\n",
                "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
                "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
                "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42)\n",
                "}\n",
                "\n",
                "print(\" CROSS-VALIDATION RESULTS (5-Fold)\")\n",
                "print(\"=\"*55)\n",
                "\n",
                "cv_results = []\n",
                "for name, model in models.items():\n",
                "    scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
                "    cv_results.append({'Model': name, 'Mean': scores.mean(), 'Std': scores.std(), 'Scores': scores})\n",
                "    print(f\"\\n{name}:\")\n",
                "    print(f\"  Fold Scores: {[f'{s:.3f}' for s in scores]}\")\n",
                "    print(f\"  Mean: {scores.mean():.3f} (+/- {scores.std()*2:.3f})\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize CV results\n",
                "fig, ax = plt.subplots(figsize=(10, 5))\n",
                "\n",
                "positions = np.arange(len(models))\n",
                "means = [r['Mean'] for r in cv_results]\n",
                "stds = [r['Std'] for r in cv_results]\n",
                "\n",
                "bars = ax.bar(positions, means, yerr=stds, capsize=5, color=['#3498db', '#2ecc71', '#e74c3c'])\n",
                "ax.set_xticks(positions)\n",
                "ax.set_xticklabels([r['Model'] for r in cv_results])\n",
                "ax.set_ylabel('Accuracy')\n",
                "ax.set_title('Model Comparison (5-Fold CV)', fontweight='bold')\n",
                "ax.set_ylim(0.7, 1.0)\n",
                "\n",
                "for bar, mean in zip(bars, means):\n",
                "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
                "            f'{mean:.3f}', ha='center', fontweight='bold')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Hyperparameter Tuning with GridSearchCV\n",
                "\n",
                "**Hyperparameters:** Settings we choose BEFORE training (e.g., `max_depth`, `n_estimators`)\n",
                "\n",
                "**GridSearchCV:** Automatically tries all combinations and finds the best!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define parameter grid\n",
                "param_grid = {\n",
                "    'n_estimators': [50, 100, 200],\n",
                "    'max_depth': [3, 5, 10, None],\n",
                "    'min_samples_split': [2, 5, 10]\n",
                "}\n",
                "\n",
                "total_combinations = 3 * 4 * 3\n",
                "print(f\" GRID SEARCH\")\n",
                "print(\"=\"*50)\n",
                "print(f\"\\nParameter Grid:\")\n",
                "for param, values in param_grid.items():\n",
                "    print(f\"  {param}: {values}\")\n",
                "print(f\"\\nTotal combinations to try: {total_combinations}\")\n",
                "print(f\"With 5-fold CV: {total_combinations * 5} model fits!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run GridSearchCV\n",
                "rf = RandomForestClassifier(random_state=42)\n",
                "\n",
                "grid_search = GridSearchCV(\n",
                "    estimator=rf,\n",
                "    param_grid=param_grid,\n",
                "    cv=5,\n",
                "    scoring='accuracy',\n",
                "    n_jobs=-1,  # Use all CPU cores\n",
                "    verbose=1\n",
                ")\n",
                "\n",
                "grid_search.fit(X_train, y_train)\n",
                "\n",
                "print(f\"\\n BEST PARAMETERS FOUND\")\n",
                "print(\"=\"*50)\n",
                "print(f\"Best Score: {grid_search.best_score_:.4f}\")\n",
                "print(f\"Best Parameters:\")\n",
                "for param, value in grid_search.best_params_.items():\n",
                "    print(f\"  {param}: {value}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compare default vs tuned model\n",
                "default_rf = RandomForestClassifier(random_state=42)\n",
                "default_rf.fit(X_train, y_train)\n",
                "default_score = default_rf.score(X_test, y_test)\n",
                "\n",
                "tuned_rf = grid_search.best_estimator_\n",
                "tuned_score = tuned_rf.score(X_test, y_test)\n",
                "\n",
                "print(f\"\\n DEFAULT vs TUNED\")\n",
                "print(\"=\"*40)\n",
                "print(f\"Default RF Accuracy: {default_score:.4f}\")\n",
                "print(f\"Tuned RF Accuracy:   {tuned_score:.4f}\")\n",
                "print(f\"Improvement:         {(tuned_score - default_score)*100:+.2f}%\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize grid search results\n",
                "results = pd.DataFrame(grid_search.cv_results_)\n",
                "\n",
                "# Pivot for heatmap\n",
                "pivot = results.pivot_table(\n",
                "    values='mean_test_score',\n",
                "    index='param_max_depth',\n",
                "    columns='param_n_estimators',\n",
                "    aggfunc='mean'\n",
                ")\n",
                "\n",
                "plt.figure(figsize=(8, 6))\n",
                "sns.heatmap(pivot, annot=True, fmt='.3f', cmap='RdYlGn', center=pivot.values.mean())\n",
                "plt.title('Grid Search Results (Accuracy)', fontweight='bold')\n",
                "plt.xlabel('n_estimators')\n",
                "plt.ylabel('max_depth')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Mini Project: Optimize Titanic Model\n",
                "\n",
                "**Goal:** Improve the Day 7 Titanic survival model using hyperparameter tuning."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create Titanic dataset (same as Day 7)\n",
                "np.random.seed(42)\n",
                "n = 800\n",
                "\n",
                "pclass = np.random.choice([1, 2, 3], n, p=[0.25, 0.25, 0.50])\n",
                "sex = np.random.choice([0, 1], n, p=[0.35, 0.65])  # 0=female, 1=male\n",
                "age = np.random.normal(30, 15, n).clip(1, 80)\n",
                "fare = np.where(pclass == 1, np.random.normal(80, 30, n),\n",
                "                np.where(pclass == 2, np.random.normal(30, 15, n),\n",
                "                         np.random.normal(15, 10, n))).clip(5, 200)\n",
                "sibsp = np.random.choice([0, 1, 2, 3], n, p=[0.6, 0.25, 0.1, 0.05])\n",
                "\n",
                "survival_prob = 0.3 + np.where(sex == 0, 0.35, -0.1) + np.where(pclass == 1, 0.25, np.where(pclass == 2, 0.1, -0.15))\n",
                "survived = (np.random.random(n) < survival_prob).astype(int)\n",
                "\n",
                "titanic = pd.DataFrame({\n",
                "    'Pclass': pclass, 'Sex': sex, 'Age': age.round(0).astype(int),\n",
                "    'Fare': fare.round(2), 'SibSp': sibsp, 'Survived': survived\n",
                "})\n",
                "\n",
                "print(f\" TITANIC DATASET\")\n",
                "print(f\"Samples: {len(titanic)} | Survived: {titanic['Survived'].mean()*100:.1f}%\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare data\n",
                "X = titanic[['Pclass', 'Sex', 'Age', 'Fare', 'SibSp']]\n",
                "y = titanic['Survived']\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                "\n",
                "# Scale features\n",
                "scaler = StandardScaler()\n",
                "X_train_scaled = scaler.fit_transform(X_train)\n",
                "X_test_scaled = scaler.transform(X_test)\n",
                "\n",
                "print(f\"Train: {len(X_train)} | Test: {len(X_test)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Baseline models\n",
                "print(\" BASELINE MODELS (Default Parameters)\")\n",
                "print(\"=\"*50)\n",
                "\n",
                "baseline_models = {\n",
                "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
                "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
                "    'Random Forest': RandomForestClassifier(random_state=42)\n",
                "}\n",
                "\n",
                "baseline_scores = {}\n",
                "for name, model in baseline_models.items():\n",
                "    if 'Logistic' in name:\n",
                "        model.fit(X_train_scaled, y_train)\n",
                "        score = model.score(X_test_scaled, y_test)\n",
                "    else:\n",
                "        model.fit(X_train, y_train)\n",
                "        score = model.score(X_test, y_test)\n",
                "    baseline_scores[name] = score\n",
                "    print(f\"{name:25}: {score:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Tune Random Forest\n",
                "print(\"\\n TUNING RANDOM FOREST...\")\n",
                "print(\"=\"*50)\n",
                "\n",
                "rf_params = {\n",
                "    'n_estimators': [50, 100, 200],\n",
                "    'max_depth': [3, 5, 10, 15],\n",
                "    'min_samples_split': [2, 5, 10],\n",
                "    'min_samples_leaf': [1, 2, 4]\n",
                "}\n",
                "\n",
                "rf_grid = GridSearchCV(\n",
                "    RandomForestClassifier(random_state=42),\n",
                "    rf_params, cv=5, scoring='f1', n_jobs=-1\n",
                ")\n",
                "rf_grid.fit(X_train, y_train)\n",
                "\n",
                "print(f\"Best CV Score: {rf_grid.best_score_:.4f}\")\n",
                "print(f\"Best Params: {rf_grid.best_params_}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Tune Decision Tree\n",
                "print(\"\\n TUNING DECISION TREE...\")\n",
                "print(\"=\"*50)\n",
                "\n",
                "dt_params = {\n",
                "    'max_depth': [3, 5, 7, 10, 15],\n",
                "    'min_samples_split': [2, 5, 10, 20],\n",
                "    'min_samples_leaf': [1, 2, 4, 8]\n",
                "}\n",
                "\n",
                "dt_grid = GridSearchCV(\n",
                "    DecisionTreeClassifier(random_state=42),\n",
                "    dt_params, cv=5, scoring='f1', n_jobs=-1\n",
                ")\n",
                "dt_grid.fit(X_train, y_train)\n",
                "\n",
                "print(f\"Best CV Score: {dt_grid.best_score_:.4f}\")\n",
                "print(f\"Best Params: {dt_grid.best_params_}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compare all models\n",
                "print(\"\\n FINAL COMPARISON: BASELINE vs TUNED\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Tuned predictions\n",
                "y_pred_rf = rf_grid.best_estimator_.predict(X_test)\n",
                "y_pred_dt = dt_grid.best_estimator_.predict(X_test)\n",
                "\n",
                "results = {\n",
                "    'Decision Tree (Default)': baseline_scores['Decision Tree'],\n",
                "    'Decision Tree (Tuned)': accuracy_score(y_test, y_pred_dt),\n",
                "    'Random Forest (Default)': baseline_scores['Random Forest'],\n",
                "    'Random Forest (Tuned)': accuracy_score(y_test, y_pred_rf),\n",
                "}\n",
                "\n",
                "print(f\"\\n{'Model':<30} {'Accuracy':>10}\")\n",
                "print(\"-\"*42)\n",
                "for model, score in results.items():\n",
                "    marker = '' if 'Tuned' in model else ''\n",
                "    print(f\"{marker} {model:<28} {score:>10.4f}\")\n",
                "\n",
                "# Improvement\n",
                "rf_improvement = results['Random Forest (Tuned)'] - results['Random Forest (Default)']\n",
                "dt_improvement = results['Decision Tree (Tuned)'] - results['Decision Tree (Default)']\n",
                "print(f\"\\nRandom Forest Improvement: {rf_improvement*100:+.2f}%\")\n",
                "print(f\"Decision Tree Improvement: {dt_improvement*100:+.2f}%\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Final evaluation of best model\n",
                "best_model = rf_grid.best_estimator_\n",
                "y_pred_final = best_model.predict(X_test)\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
                "\n",
                "# Confusion Matrix\n",
                "cm = confusion_matrix(y_test, y_pred_final)\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
                "            xticklabels=['Died', 'Survived'], yticklabels=['Died', 'Survived'])\n",
                "axes[0].set_title('Tuned Random Forest - Confusion Matrix', fontweight='bold')\n",
                "axes[0].set_xlabel('Predicted')\n",
                "axes[0].set_ylabel('Actual')\n",
                "\n",
                "# Feature Importance\n",
                "importance = pd.DataFrame({\n",
                "    'Feature': X.columns,\n",
                "    'Importance': best_model.feature_importances_\n",
                "}).sort_values('Importance', ascending=True)\n",
                "\n",
                "axes[1].barh(importance['Feature'], importance['Importance'], color=plt.cm.viridis(np.linspace(0.3, 0.9, 5)))\n",
                "axes[1].set_title('Feature Importance (Tuned RF)', fontweight='bold')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nDetailed Classification Report:\")\n",
                "print(classification_report(y_test, y_pred_final, target_names=['Died', 'Survived']))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Final Summary\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\" DAY 9 COMPLETE!\")\n",
                "print(\"=\"*60)\n",
                "print(\"\"\"\n",
                " KEY TAKEAWAYS:\n",
                "\n",
                " 1. EVALUATION METRICS\n",
                "    - Accuracy: Overall correctness\n",
                "    - Precision: Quality of positive predictions\n",
                "    - Recall: Coverage of actual positives\n",
                "    - F1: Harmonic mean of P and R\n",
                "\n",
                " 2. CROSS-VALIDATION\n",
                "    - More reliable than single split\n",
                "    - K-Fold: Train K times, average results\n",
                "    - Detects overfitting\n",
                "\n",
                " 3. HYPERPARAMETER TUNING\n",
                "    - GridSearchCV: Try all combinations\n",
                "    - Automatically uses cross-validation\n",
                "    - Always tune with CV, evaluate on test set\n",
                "\n",
                " 4. BEST PRACTICES\n",
                "    - Use CV for model selection\n",
                "    - Tune on validation, test on holdout\n",
                "    - Choose metrics based on problem\n",
                "\"\"\")\n",
                "print(\"=\"*60)\n",
                "print(\" Next: Day 10 - Capstone Project!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Practice Exercises\n",
                "\n",
                "1. Try `RandomizedSearchCV` for faster tuning\n",
                "2. Tune Logistic Regression (C, penalty)\n",
                "3. Use `StratifiedKFold` for imbalanced data\n",
                "4. Plot ROC curves for model comparison\n",
                "\n",
                "---\n",
                "**Next Up:** Day 10 - Capstone Project!"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.13.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
