{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Day 6: Introduction to Machine Learning - Linear Regression\n",
                "\n",
                "Welcome to your first real Machine Learning lesson! Today we'll learn the fundamentals of ML and implement our first predictive model using **Linear Regression**.\n",
                "\n",
                "### What is Machine Learning?\n",
                "- Teaching computers to learn patterns from data without being explicitly programmed.\n",
                "- Instead of writing rules, we let the algorithm **discover rules** from examples.\n",
                "- The model gets better with more data.\n",
                "\n",
                "### Topics Covered:\n",
                "1. **The ML Pipeline**\n",
                "2. **Types of Machine Learning**\n",
                "3. **Linear Regression Theory**\n",
                "4. **Train/Test Split**\n",
                "5. **Building Your First Model**\n",
                "6. **Loss Functions & Evaluation Metrics**\n",
                "7. **Making Predictions**\n",
                "8. **Mini Project: House Price Prediction**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import essential libraries\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# Scikit-Learn imports\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.linear_model import LinearRegression\n",
                "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "\n",
                "# Settings\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "plt.rcParams['figure.figsize'] = (10, 6)\n",
                "np.random.seed(42)\n",
                "\n",
                "print(\"Libraries loaded successfully!\")\n",
                "print(\"Ready to start your ML journey!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. The Machine Learning Pipeline\n",
                "\n",
                "Every ML project follows a similar workflow:\n",
                "\n",
                "```\n",
                "Data Collection → Data Preprocessing → Feature Engineering → Train/Test Split → \n",
                "Model Training → Evaluation → Prediction → Deployment\n",
                "```\n",
                "\n",
                "Let's understand each step!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visual representation of the ML Pipeline\n",
                "pipeline_steps = [\n",
                "    ('1. Data Collection', 'Gather raw data from sources'),\n",
                "    ('2. Data Cleaning', 'Handle missing values, remove duplicates'),\n",
                "    ('3. Feature Engineering', 'Create/select meaningful features'),\n",
                "    ('4. Train/Test Split', 'Divide data for training and evaluation'),\n",
                "    ('5. Model Training', 'Fit the algorithm on training data'),\n",
                "    ('6. Evaluation', 'Measure model performance on test data'),\n",
                "    ('7. Prediction', 'Make predictions on new, unseen data'),\n",
                "]\n",
                "\n",
                "print(\"THE MACHINE LEARNING PIPELINE\")\n",
                "print(\"=\" * 60)\n",
                "for step, description in pipeline_steps:\n",
                "    print(f\"\\n{step}\")\n",
                "    print(f\"   → {description}\")\n",
                "print(\"\\n\" + \"=\" * 60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Types of Machine Learning\n",
                "\n",
                "| Type | Description | Examples |\n",
                "|------|-------------|----------|\n",
                "| **Supervised Learning** | Learn from labeled data | Regression, Classification |\n",
                "| **Unsupervised Learning** | Find patterns in unlabeled data | Clustering, Dimensionality Reduction |\n",
                "| **Reinforcement Learning** | Learn through trial and error | Game AI, Robotics |\n",
                "\n",
                "Today we focus on **Supervised Learning - Regression** (predicting continuous values)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize the difference between Classification and Regression\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Classification Example\n",
                "np.random.seed(42)\n",
                "class_a_x = np.random.normal(2, 0.5, 50)\n",
                "class_a_y = np.random.normal(2, 0.5, 50)\n",
                "class_b_x = np.random.normal(4, 0.5, 50)\n",
                "class_b_y = np.random.normal(4, 0.5, 50)\n",
                "\n",
                "axes[0].scatter(class_a_x, class_a_y, c='#3498db', label='Class A', s=60, alpha=0.7)\n",
                "axes[0].scatter(class_b_x, class_b_y, c='#e74c3c', label='Class B', s=60, alpha=0.7)\n",
                "axes[0].plot([0, 6], [6, 0], 'k--', linewidth=2, label='Decision Boundary')\n",
                "axes[0].set_title('Classification: Predicting Categories', fontsize=14, fontweight='bold')\n",
                "axes[0].set_xlabel('Feature 1')\n",
                "axes[0].set_ylabel('Feature 2')\n",
                "axes[0].legend()\n",
                "axes[0].set_xlim(0, 6)\n",
                "axes[0].set_ylim(0, 6)\n",
                "\n",
                "# Regression Example\n",
                "x_reg = np.linspace(0, 10, 50)\n",
                "y_reg = 2 * x_reg + 5 + np.random.normal(0, 2, 50)\n",
                "\n",
                "axes[1].scatter(x_reg, y_reg, c='#2ecc71', s=60, alpha=0.7, label='Data Points')\n",
                "axes[1].plot(x_reg, 2 * x_reg + 5, 'r-', linewidth=2, label='Regression Line')\n",
                "axes[1].set_title('Regression: Predicting Continuous Values', fontsize=14, fontweight='bold')\n",
                "axes[1].set_xlabel('Feature (e.g., Square Feet)')\n",
                "axes[1].set_ylabel('Target (e.g., Price)')\n",
                "axes[1].legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Linear Regression Theory\n",
                "\n",
                "### The Concept\n",
                "Linear Regression finds the best-fitting straight line through your data points.\n",
                "\n",
                "### The Equation\n",
                "```\n",
                "y = mx + b\n",
                "```\n",
                "Or in ML terminology:\n",
                "```\n",
                "y = w₁x₁ + w₂x₂ + ... + wₙxₙ + b\n",
                "```\n",
                "\n",
                "Where:\n",
                "- **y** = Target variable (what we're predicting)\n",
                "- **x** = Features (input variables)\n",
                "- **w** = Weights (coefficients - learned from data)\n",
                "- **b** = Bias/Intercept (where the line crosses Y-axis)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Understanding Linear Regression Visually\n",
                "np.random.seed(42)\n",
                "\n",
                "# Create sample data: Square Feet vs Price\n",
                "sqft = np.array([1000, 1200, 1400, 1600, 1800, 2000, 2200, 2400, 2600, 2800])\n",
                "price = np.array([150, 180, 210, 240, 280, 310, 350, 380, 420, 450])\n",
                "\n",
                "# Add some noise\n",
                "price_noisy = price + np.random.normal(0, 15, len(price))\n",
                "\n",
                "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
                "\n",
                "# Plot 1: Just the data\n",
                "axes[0].scatter(sqft, price_noisy, c='#3498db', s=100, edgecolors='white')\n",
                "axes[0].set_title('Step 1: Our Data', fontsize=12, fontweight='bold')\n",
                "axes[0].set_xlabel('Square Feet')\n",
                "axes[0].set_ylabel('Price ($K)')\n",
                "\n",
                "# Plot 2: Finding the best line\n",
                "axes[1].scatter(sqft, price_noisy, c='#3498db', s=100, edgecolors='white')\n",
                "# Try different lines\n",
                "for slope, intercept, color, alpha in [(0.1, 100, 'gray', 0.3), (0.2, 50, 'gray', 0.3), (0.15, 10, 'red', 1)]:\n",
                "    y_line = slope * sqft + intercept\n",
                "    axes[1].plot(sqft, y_line, color=color, linewidth=2, alpha=alpha)\n",
                "axes[1].set_title('Step 2: Finding Best Fit Line', fontsize=12, fontweight='bold')\n",
                "axes[1].set_xlabel('Square Feet')\n",
                "axes[1].set_ylabel('Price ($K)')\n",
                "\n",
                "# Plot 3: Best fit with residuals\n",
                "# Calculate best fit\n",
                "z = np.polyfit(sqft, price_noisy, 1)\n",
                "p = np.poly1d(z)\n",
                "y_pred = p(sqft)\n",
                "\n",
                "axes[2].scatter(sqft, price_noisy, c='#3498db', s=100, edgecolors='white', label='Actual')\n",
                "axes[2].plot(sqft, y_pred, 'r-', linewidth=2, label=f'Best Fit: y = {z[0]:.3f}x + {z[1]:.1f}')\n",
                "# Draw residuals (errors)\n",
                "for xi, yi, yi_pred in zip(sqft, price_noisy, y_pred):\n",
                "    axes[2].plot([xi, xi], [yi, yi_pred], 'g--', alpha=0.5)\n",
                "axes[2].set_title('Step 3: Best Line with Errors', fontsize=12, fontweight='bold')\n",
                "axes[2].set_xlabel('Square Feet')\n",
                "axes[2].set_ylabel('Price ($K)')\n",
                "axes[2].legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\nThe Best Fit Equation: Price = {z[0]:.4f} × SqFt + {z[1]:.2f}\")\n",
                "print(f\"This means: For every 1 sqft increase, price increases by ${z[0]*1000:.2f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Train/Test Split - Why It's Critical!\n",
                "\n",
                "### The Problem: Overfitting\n",
                "If we train and test on the same data, the model might just \"memorize\" the data instead of learning patterns.\n",
                "\n",
                "### The Solution: Split Your Data\n",
                "- **Training Set (70-80%)**: Data the model learns from\n",
                "- **Test Set (20-30%)**: Data the model has never seen - used for evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a larger dataset\n",
                "np.random.seed(42)\n",
                "n_samples = 200\n",
                "\n",
                "# Features\n",
                "X = np.random.uniform(800, 3500, n_samples).reshape(-1, 1)  # Square feet\n",
                "\n",
                "# Target (with realistic relationship)\n",
                "y = 50000 + 150 * X.flatten() + np.random.normal(0, 30000, n_samples)  # Price\n",
                "\n",
                "print(f\"Total samples: {len(X)}\")\n",
                "print(f\"Feature shape: {X.shape}\")\n",
                "print(f\"Target shape: {y.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# The Train/Test Split\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, \n",
                "    test_size=0.2,      # 20% for testing\n",
                "    random_state=42     # For reproducibility\n",
                ")\n",
                "\n",
                "print(\"DATA SPLIT SUMMARY\")\n",
                "print(\"=\" * 40)\n",
                "print(f\"Training set size: {len(X_train)} samples ({len(X_train)/len(X)*100:.0f}%)\")\n",
                "print(f\"Test set size: {len(X_test)} samples ({len(X_test)/len(X)*100:.0f}%)\")\n",
                "print(\"\\nTraining Data:\")\n",
                "print(f\"  X_train shape: {X_train.shape}\")\n",
                "print(f\"  y_train shape: {y_train.shape}\")\n",
                "print(\"\\nTest Data:\")\n",
                "print(f\"  X_test shape: {X_test.shape}\")\n",
                "print(f\"  y_test shape: {y_test.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize the split\n",
                "fig, ax = plt.subplots(figsize=(10, 6))\n",
                "\n",
                "ax.scatter(X_train, y_train, c='#3498db', alpha=0.6, label=f'Training Data ({len(X_train)} samples)', s=50)\n",
                "ax.scatter(X_test, y_test, c='#e74c3c', alpha=0.8, label=f'Test Data ({len(X_test)} samples)', s=50, marker='s')\n",
                "\n",
                "ax.set_title('Train/Test Split Visualization', fontsize=14, fontweight='bold')\n",
                "ax.set_xlabel('Square Feet', fontsize=12)\n",
                "ax.set_ylabel('Price ($)', fontsize=12)\n",
                "ax.legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Building Your First ML Model!\n",
                "\n",
                "Using Scikit-Learn, building a model is just 3 lines of code:\n",
                "1. **Create** the model\n",
                "2. **Fit** (train) on data\n",
                "3. **Predict** on new data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 1: Create the model\n",
                "model = LinearRegression()\n",
                "\n",
                "print(\"Model created!\")\n",
                "print(f\"Model type: {type(model).__name__}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 2: Train (fit) the model\n",
                "model.fit(X_train, y_train)\n",
                "\n",
                "print(\"Model trained!\")\n",
                "print(\"\\n LEARNED PARAMETERS:\")\n",
                "print(f\"   Coefficient (slope): {model.coef_[0]:.4f}\")\n",
                "print(f\"   Intercept (bias): {model.intercept_:.4f}\")\n",
                "print(f\"\\n The Equation: Price = {model.coef_[0]:.2f} × SqFt + {model.intercept_:.2f}\")\n",
                "print(f\"\\n Interpretation: For every 1 sqft increase, price goes up by ${model.coef_[0]:.2f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 3: Make predictions\n",
                "y_pred_train = model.predict(X_train)  # Predictions on training data\n",
                "y_pred_test = model.predict(X_test)    # Predictions on test data\n",
                "\n",
                "print(\"Predictions made!\")\n",
                "print(\"\\nSample Predictions vs Actual (Test Set):\")\n",
                "print(\"-\" * 50)\n",
                "print(f\"{'SqFt':>10} | {'Actual':>12} | {'Predicted':>12} | {'Error':>10}\")\n",
                "print(\"-\" * 50)\n",
                "for i in range(5):\n",
                "    sqft = X_test[i][0]\n",
                "    actual = y_test[i]\n",
                "    predicted = y_pred_test[i]\n",
                "    error = actual - predicted\n",
                "    print(f\"{sqft:>10.0f} | ${actual:>10,.0f} | ${predicted:>10,.0f} | ${error:>+9,.0f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize the model's predictions\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Plot 1: Regression line on training data\n",
                "axes[0].scatter(X_train, y_train, c='#3498db', alpha=0.6, label='Training Data')\n",
                "axes[0].plot(np.sort(X_train.flatten()), model.predict(np.sort(X_train, axis=0)), \n",
                "             'r-', linewidth=2, label='Model')\n",
                "axes[0].set_title('Model Fit on Training Data', fontsize=12, fontweight='bold')\n",
                "axes[0].set_xlabel('Square Feet')\n",
                "axes[0].set_ylabel('Price ($)')\n",
                "axes[0].legend()\n",
                "\n",
                "# Plot 2: Predictions vs Actual on test data\n",
                "axes[1].scatter(y_test, y_pred_test, c='#2ecc71', alpha=0.6)\n",
                "# Perfect prediction line\n",
                "min_val = min(y_test.min(), y_pred_test.min())\n",
                "max_val = max(y_test.max(), y_pred_test.max())\n",
                "axes[1].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
                "axes[1].set_title('Predicted vs Actual (Test Set)', fontsize=12, fontweight='bold')\n",
                "axes[1].set_xlabel('Actual Price ($)')\n",
                "axes[1].set_ylabel('Predicted Price ($)')\n",
                "axes[1].legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Loss Functions & Evaluation Metrics\n",
                "\n",
                "How do we know if our model is any good? We use **metrics**!\n",
                "\n",
                "### Common Regression Metrics:\n",
                "\n",
                "| Metric | Full Name | What It Measures | Ideal Value |\n",
                "|--------|-----------|------------------|-------------|\n",
                "| **MSE** | Mean Squared Error | Average of squared errors | Lower = Better |\n",
                "| **RMSE** | Root Mean Squared Error | Square root of MSE (same units as target) | Lower = Better |\n",
                "| **MAE** | Mean Absolute Error | Average of absolute errors | Lower = Better |\n",
                "| **R²** | R-Squared (Coefficient of Determination) | Variance explained by model | Closer to 1 = Better |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Understanding Loss Functions\n",
                "print(\" LOSS FUNCTIONS EXPLAINED\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "# Example predictions and actuals\n",
                "actual = np.array([100, 200, 300, 400, 500])\n",
                "predicted = np.array([110, 190, 320, 380, 510])\n",
                "errors = actual - predicted\n",
                "\n",
                "print(\"\\nSimple Example:\")\n",
                "print(f\"Actual:    {actual}\")\n",
                "print(f\"Predicted: {predicted}\")\n",
                "print(f\"Errors:    {errors}\")\n",
                "\n",
                "# Calculate metrics step by step\n",
                "print(\"\\n--- Mean Absolute Error (MAE) ---\")\n",
                "mae = np.mean(np.abs(errors))\n",
                "print(f\"MAE = mean(|errors|) = mean({np.abs(errors)}) = {mae:.2f}\")\n",
                "print(\"Interpretation: On average, predictions are off by $\" + f\"{mae:.0f}\")\n",
                "\n",
                "print(\"\\n--- Mean Squared Error (MSE) ---\")\n",
                "squared_errors = errors ** 2\n",
                "mse = np.mean(squared_errors)\n",
                "print(f\"MSE = mean(errors²) = mean({squared_errors}) = {mse:.2f}\")\n",
                "print(\"Note: Penalizes large errors more heavily\")\n",
                "\n",
                "print(\"\\n--- Root Mean Squared Error (RMSE) ---\")\n",
                "rmse = np.sqrt(mse)\n",
                "print(f\"RMSE = √MSE = √{mse:.2f} = {rmse:.2f}\")\n",
                "print(\"Interpretation: Typical prediction error is about $\" + f\"{rmse:.0f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate metrics for our model\n",
                "print(\" MODEL EVALUATION\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "# Training metrics\n",
                "train_mse = mean_squared_error(y_train, y_pred_train)\n",
                "train_rmse = np.sqrt(train_mse)\n",
                "train_mae = mean_absolute_error(y_train, y_pred_train)\n",
                "train_r2 = r2_score(y_train, y_pred_train)\n",
                "\n",
                "# Test metrics\n",
                "test_mse = mean_squared_error(y_test, y_pred_test)\n",
                "test_rmse = np.sqrt(test_mse)\n",
                "test_mae = mean_absolute_error(y_test, y_pred_test)\n",
                "test_r2 = r2_score(y_test, y_pred_test)\n",
                "\n",
                "print(\"\\n TRAINING SET METRICS:\")\n",
                "print(f\"   MSE:  {train_mse:,.0f}\")\n",
                "print(f\"   RMSE: ${train_rmse:,.0f}\")\n",
                "print(f\"   MAE:  ${train_mae:,.0f}\")\n",
                "print(f\"   R²:   {train_r2:.4f} ({train_r2*100:.1f}% variance explained)\")\n",
                "\n",
                "print(\"\\n TEST SET METRICS:\")\n",
                "print(f\"   MSE:  {test_mse:,.0f}\")\n",
                "print(f\"   RMSE: ${test_rmse:,.0f}\")\n",
                "print(f\"   MAE:  ${test_mae:,.0f}\")\n",
                "print(f\"   R²:   {test_r2:.4f} ({test_r2*100:.1f}% variance explained)\")\n",
                "\n",
                "print(\"\\n INTERPRETATION:\")\n",
                "print(f\"   On average, our predictions are off by ${test_mae:,.0f}\")\n",
                "print(f\"   The model explains {test_r2*100:.1f}% of the variance in house prices\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize the residuals (errors)\n",
                "residuals = y_test - y_pred_test\n",
                "\n",
                "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
                "\n",
                "# Plot 1: Residuals vs Predicted\n",
                "axes[0].scatter(y_pred_test, residuals, c='#3498db', alpha=0.6)\n",
                "axes[0].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
                "axes[0].set_title('Residuals vs Predicted', fontsize=12, fontweight='bold')\n",
                "axes[0].set_xlabel('Predicted Price')\n",
                "axes[0].set_ylabel('Residual (Error)')\n",
                "\n",
                "# Plot 2: Residual Distribution\n",
                "axes[1].hist(residuals, bins=20, color='#2ecc71', edgecolor='white', alpha=0.7)\n",
                "axes[1].axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
                "axes[1].set_title('Residual Distribution', fontsize=12, fontweight='bold')\n",
                "axes[1].set_xlabel('Residual')\n",
                "axes[1].set_ylabel('Frequency')\n",
                "\n",
                "# Plot 3: Metrics comparison\n",
                "metrics = ['RMSE', 'MAE']\n",
                "train_values = [train_rmse, train_mae]\n",
                "test_values = [test_rmse, test_mae]\n",
                "\n",
                "x = np.arange(len(metrics))\n",
                "width = 0.35\n",
                "\n",
                "axes[2].bar(x - width/2, train_values, width, label='Train', color='#3498db')\n",
                "axes[2].bar(x + width/2, test_values, width, label='Test', color='#e74c3c')\n",
                "axes[2].set_title('Train vs Test Metrics', fontsize=12, fontweight='bold')\n",
                "axes[2].set_xticks(x)\n",
                "axes[2].set_xticklabels(metrics)\n",
                "axes[2].set_ylabel('Error ($)')\n",
                "axes[2].legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Making Predictions on New Data\n",
                "\n",
                "Now let's use our trained model to predict prices for houses we've never seen!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Predict prices for new houses\n",
                "new_houses = np.array([[1500], [2000], [2500], [3000], [3500]])\n",
                "\n",
                "predictions = model.predict(new_houses)\n",
                "\n",
                "print(\" PRICE PREDICTIONS FOR NEW HOUSES\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"\\nModel: Price = ${model.coef_[0]:.2f} × SqFt + ${model.intercept_:,.2f}\")\n",
                "print(\"\\n\" + \"-\" * 50)\n",
                "print(f\"{'Square Feet':^15} | {'Predicted Price':^20}\")\n",
                "print(\"-\" * 50)\n",
                "\n",
                "for sqft, price in zip(new_houses.flatten(), predictions):\n",
                "    print(f\"{sqft:^15,.0f} | ${price:^18,.0f}\")\n",
                "\n",
                "print(\"-\" * 50)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Interactive prediction visualization\n",
                "fig, ax = plt.subplots(figsize=(12, 6))\n",
                "\n",
                "# Plot original data\n",
                "ax.scatter(X, y, c='#3498db', alpha=0.4, label='Original Data')\n",
                "\n",
                "# Plot regression line\n",
                "X_line = np.linspace(500, 4000, 100).reshape(-1, 1)\n",
                "y_line = model.predict(X_line)\n",
                "ax.plot(X_line, y_line, 'r-', linewidth=2, label='Model')\n",
                "\n",
                "# Plot new predictions\n",
                "ax.scatter(new_houses, predictions, c='#2ecc71', s=200, marker='*', \n",
                "           edgecolors='black', linewidth=2, label='New Predictions', zorder=5)\n",
                "\n",
                "# Add price labels\n",
                "for sqft, price in zip(new_houses.flatten(), predictions):\n",
                "    ax.annotate(f'${price/1000:.0f}K', \n",
                "                xy=(sqft, price), \n",
                "                xytext=(sqft + 100, price + 30000),\n",
                "                fontsize=10, fontweight='bold')\n",
                "\n",
                "ax.set_title('House Price Predictions', fontsize=14, fontweight='bold')\n",
                "ax.set_xlabel('Square Feet', fontsize=12)\n",
                "ax.set_ylabel('Price ($)', fontsize=12)\n",
                "ax.legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Mini Project: Complete House Price Prediction System\n",
                "\n",
                "**Goal:** Build a complete ML system to predict house prices using multiple features.\n",
                "\n",
                "Let's put everything together!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a comprehensive housing dataset\n",
                "np.random.seed(42)\n",
                "n = 500\n",
                "\n",
                "# Generate features\n",
                "sqft = np.random.normal(2000, 500, n).clip(800, 4500)\n",
                "bedrooms = np.random.choice([1, 2, 3, 4, 5], n, p=[0.05, 0.15, 0.40, 0.30, 0.10])\n",
                "bathrooms = np.random.choice([1, 1.5, 2, 2.5, 3], n, p=[0.15, 0.20, 0.35, 0.20, 0.10])\n",
                "age = np.random.uniform(0, 50, n)\n",
                "has_garage = np.random.choice([0, 1], n, p=[0.3, 0.7])\n",
                "\n",
                "# Generate target with realistic relationships\n",
                "price = (\n",
                "    50000 +                          # Base price\n",
                "    sqft * 150 +                     # $150 per sqft\n",
                "    bedrooms * 20000 +               # $20K per bedroom\n",
                "    bathrooms * 15000 +              # $15K per bathroom\n",
                "    (50 - age) * 1000 +              # Newer = more expensive\n",
                "    has_garage * 30000 +             # $30K for garage\n",
                "    np.random.normal(0, 25000, n)    # Noise\n",
                ").clip(100000, 1000000)\n",
                "\n",
                "# Create DataFrame\n",
                "housing_df = pd.DataFrame({\n",
                "    'SqFt': sqft.astype(int),\n",
                "    'Bedrooms': bedrooms,\n",
                "    'Bathrooms': bathrooms,\n",
                "    'Age': age.astype(int),\n",
                "    'Has_Garage': has_garage,\n",
                "    'Price': price.astype(int)\n",
                "})\n",
                "\n",
                "print(\" HOUSING DATASET\")\n",
                "print(\"=\" * 60)\n",
                "print(f\"Total Records: {len(housing_df)}\")\n",
                "print(f\"Features: {list(housing_df.columns[:-1])}\")\n",
                "print(f\"Target: Price\")\n",
                "print(\"\\nFirst 10 Records:\")\n",
                "print(housing_df.head(10))\n",
                "print(\"\\nStatistical Summary:\")\n",
                "print(housing_df.describe().round(0))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 1: Exploratory Data Analysis\n",
                "print(\" STEP 1: EXPLORATORY DATA ANALYSIS\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "# Correlation analysis\n",
                "correlations = housing_df.corr()['Price'].drop('Price').sort_values(ascending=False)\n",
                "print(\"\\nCorrelation with Price:\")\n",
                "for feature, corr in correlations.items():\n",
                "    bar = '' * int(abs(corr) * 20)\n",
                "    print(f\"  {feature:12}: {corr:+.3f} {bar}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize correlations\n",
                "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
                "\n",
                "features = ['SqFt', 'Bedrooms', 'Bathrooms', 'Age', 'Has_Garage']\n",
                "\n",
                "for i, feature in enumerate(features):\n",
                "    row, col = i // 3, i % 3\n",
                "    \n",
                "    if feature in ['Has_Garage']:\n",
                "        # Box plot for categorical\n",
                "        housing_df.boxplot(column='Price', by=feature, ax=axes[row, col])\n",
                "        axes[row, col].set_title(f'Price by {feature}', fontsize=11)\n",
                "    else:\n",
                "        # Scatter for continuous\n",
                "        axes[row, col].scatter(housing_df[feature], housing_df['Price'], alpha=0.5, c='#3498db')\n",
                "        # Add trend line\n",
                "        z = np.polyfit(housing_df[feature], housing_df['Price'], 1)\n",
                "        p = np.poly1d(z)\n",
                "        x_line = np.linspace(housing_df[feature].min(), housing_df[feature].max(), 100)\n",
                "        axes[row, col].plot(x_line, p(x_line), 'r-', linewidth=2)\n",
                "        axes[row, col].set_title(f'Price vs {feature}', fontsize=11)\n",
                "    \n",
                "    axes[row, col].set_xlabel(feature)\n",
                "    axes[row, col].set_ylabel('Price')\n",
                "\n",
                "# Correlation heatmap\n",
                "sns.heatmap(housing_df.corr(), annot=True, cmap='RdYlBu_r', center=0, \n",
                "            ax=axes[1, 2], fmt='.2f', square=True)\n",
                "axes[1, 2].set_title('Correlation Matrix', fontsize=11)\n",
                "\n",
                "plt.suptitle('Feature Analysis', fontsize=14, fontweight='bold', y=1.02)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 2: Prepare Data\n",
                "print(\" STEP 2: PREPARE DATA\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "# Features and Target\n",
                "X = housing_df[['SqFt', 'Bedrooms', 'Bathrooms', 'Age', 'Has_Garage']]\n",
                "y = housing_df['Price']\n",
                "\n",
                "# Split data\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                "\n",
                "print(f\"\\nFeatures used: {list(X.columns)}\")\n",
                "print(f\"Training samples: {len(X_train)}\")\n",
                "print(f\"Test samples: {len(X_test)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 3: Train the Model\n",
                "print(\" STEP 3: TRAIN THE MODEL\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "# Create and train model\n",
                "model = LinearRegression()\n",
                "model.fit(X_train, y_train)\n",
                "\n",
                "print(\"\\n Model Trained Successfully!\")\n",
                "print(\"\\n LEARNED COEFFICIENTS:\")\n",
                "print(\"-\" * 40)\n",
                "\n",
                "for feature, coef in zip(X.columns, model.coef_):\n",
                "    print(f\"  {feature:12}: ${coef:+,.2f}\")\n",
                "print(f\"  {'Intercept':12}: ${model.intercept_:+,.2f}\")\n",
                "\n",
                "print(\"\\n INTERPRETATION:\")\n",
                "print(f\"  • Each additional sqft adds ${model.coef_[0]:.2f} to the price\")\n",
                "print(f\"  • Each bedroom adds ${model.coef_[1]:,.0f} to the price\")\n",
                "print(f\"  • Each bathroom adds ${model.coef_[2]:,.0f} to the price\")\n",
                "print(f\"  • Each year of age {'decreases' if model.coef_[3] < 0 else 'increases'} price by ${abs(model.coef_[3]):,.0f}\")\n",
                "print(f\"  • Having a garage adds ${model.coef_[4]:,.0f} to the price\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 4: Evaluate the Model\n",
                "print(\" STEP 4: EVALUATE THE MODEL\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "# Make predictions\n",
                "y_pred_train = model.predict(X_train)\n",
                "y_pred_test = model.predict(X_test)\n",
                "\n",
                "# Calculate metrics\n",
                "metrics = {\n",
                "    'Training': {\n",
                "        'RMSE': np.sqrt(mean_squared_error(y_train, y_pred_train)),\n",
                "        'MAE': mean_absolute_error(y_train, y_pred_train),\n",
                "        'R²': r2_score(y_train, y_pred_train)\n",
                "    },\n",
                "    'Test': {\n",
                "        'RMSE': np.sqrt(mean_squared_error(y_test, y_pred_test)),\n",
                "        'MAE': mean_absolute_error(y_test, y_pred_test),\n",
                "        'R²': r2_score(y_test, y_pred_test)\n",
                "    }\n",
                "}\n",
                "\n",
                "print(\"\\n\" + \"-\" * 50)\n",
                "print(f\"{'Metric':^15} | {'Training':^15} | {'Test':^15}\")\n",
                "print(\"-\" * 50)\n",
                "print(f\"{'RMSE':^15} | ${metrics['Training']['RMSE']:>12,.0f} | ${metrics['Test']['RMSE']:>12,.0f}\")\n",
                "print(f\"{'MAE':^15} | ${metrics['Training']['MAE']:>12,.0f} | ${metrics['Test']['MAE']:>12,.0f}\")\n",
                "print(f\"{'R²':^15} | {metrics['Training']['R²']:>13.4f} | {metrics['Test']['R²']:>13.4f}\")\n",
                "print(\"-\" * 50)\n",
                "\n",
                "print(f\"\\n MODEL PERFORMANCE SUMMARY:\")\n",
                "print(f\"   • The model explains {metrics['Test']['R²']*100:.1f}% of price variance\")\n",
                "print(f\"   • Average prediction error: ${metrics['Test']['MAE']:,.0f}\")\n",
                "print(f\"   • Typical error range: ±${metrics['Test']['RMSE']:,.0f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualization of model performance\n",
                "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
                "\n",
                "# 1. Actual vs Predicted\n",
                "axes[0].scatter(y_test, y_pred_test, c='#3498db', alpha=0.5)\n",
                "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', linewidth=2)\n",
                "axes[0].set_title('Predicted vs Actual Prices', fontsize=12, fontweight='bold')\n",
                "axes[0].set_xlabel('Actual Price ($)')\n",
                "axes[0].set_ylabel('Predicted Price ($)')\n",
                "\n",
                "# 2. Residual Distribution\n",
                "residuals = y_test - y_pred_test\n",
                "axes[1].hist(residuals, bins=30, color='#2ecc71', edgecolor='white', alpha=0.7)\n",
                "axes[1].axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
                "axes[1].set_title('Residual Distribution', fontsize=12, fontweight='bold')\n",
                "axes[1].set_xlabel('Prediction Error ($)')\n",
                "axes[1].set_ylabel('Frequency')\n",
                "\n",
                "# 3. Feature Importance\n",
                "feature_importance = pd.DataFrame({\n",
                "    'Feature': X.columns,\n",
                "    'Coefficient': np.abs(model.coef_)\n",
                "}).sort_values('Coefficient', ascending=True)\n",
                "\n",
                "colors = plt.cm.RdYlGn(np.linspace(0.2, 0.8, len(feature_importance)))\n",
                "axes[2].barh(feature_importance['Feature'], feature_importance['Coefficient'], color=colors)\n",
                "axes[2].set_title('Feature Importance (|Coefficient|)', fontsize=12, fontweight='bold')\n",
                "axes[2].set_xlabel('Absolute Coefficient Value')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 5: Make Predictions on New Houses\n",
                "print(\" STEP 5: PREDICT NEW HOUSE PRICES\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "# Create new houses to predict\n",
                "new_houses = pd.DataFrame({\n",
                "    'SqFt': [1500, 2000, 2500, 3000, 3500],\n",
                "    'Bedrooms': [2, 3, 3, 4, 5],\n",
                "    'Bathrooms': [1.5, 2, 2.5, 3, 3],\n",
                "    'Age': [5, 10, 15, 0, 20],\n",
                "    'Has_Garage': [1, 1, 1, 1, 0]\n",
                "})\n",
                "\n",
                "# Make predictions\n",
                "predictions = model.predict(new_houses)\n",
                "\n",
                "# Display results\n",
                "results = new_houses.copy()\n",
                "results['Predicted_Price'] = predictions.astype(int)\n",
                "\n",
                "print(\"\\n NEW HOUSE PRICE PREDICTIONS:\")\n",
                "print(results.to_string(index=False))\n",
                "\n",
                "print(\"\\n SAMPLE CALCULATION (House #1):\")\n",
                "house = new_houses.iloc[0]\n",
                "calculated = (\n",
                "    model.intercept_ +\n",
                "    model.coef_[0] * house['SqFt'] +\n",
                "    model.coef_[1] * house['Bedrooms'] +\n",
                "    model.coef_[2] * house['Bathrooms'] +\n",
                "    model.coef_[3] * house['Age'] +\n",
                "    model.coef_[4] * house['Has_Garage']\n",
                ")\n",
                "print(f\"   Intercept: ${model.intercept_:,.2f}\")\n",
                "print(f\"   + SqFt ({house['SqFt']}): ${model.coef_[0] * house['SqFt']:,.2f}\")\n",
                "print(f\"   + Bedrooms ({house['Bedrooms']}): ${model.coef_[1] * house['Bedrooms']:,.2f}\")\n",
                "print(f\"   + Bathrooms ({house['Bathrooms']}): ${model.coef_[2] * house['Bathrooms']:,.2f}\")\n",
                "print(f\"   + Age ({house['Age']}): ${model.coef_[3] * house['Age']:,.2f}\")\n",
                "print(f\"   + Garage ({house['Has_Garage']}): ${model.coef_[4] * house['Has_Garage']:,.2f}\")\n",
                "print(f\"   = Total: ${calculated:,.2f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Final Summary\n",
                "print(\"\\n\" + \"=\" * 70)\n",
                "print(\" DAY 6 COMPLETE: LINEAR REGRESSION SUMMARY\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "print(f\"\"\"\n",
                " WHAT WE LEARNED:\n",
                "\n",
                "   1. THE ML PIPELINE\n",
                "      Data → Preprocessing → Split → Train → Evaluate → Predict\n",
                "\n",
                "   2. LINEAR REGRESSION\n",
                "      y = w₁x₁ + w₂x₂ + ... + wₙxₙ + b\n",
                "      Finds the best line (or hyperplane) through your data\n",
                "\n",
                "   3. TRAIN/TEST SPLIT\n",
                "      Prevents overfitting by evaluating on unseen data\n",
                "\n",
                "   4. EVALUATION METRICS\n",
                "      • RMSE: Typical prediction error\n",
                "      • MAE: Average absolute error\n",
                "      • R²: Variance explained (0-1, higher is better)\n",
                "\n",
                "   5. SCIKIT-LEARN WORKFLOW\n",
                "      model = LinearRegression()\n",
                "      model.fit(X_train, y_train)\n",
                "      predictions = model.predict(X_test)\n",
                "\n",
                " OUR MODEL'S PERFORMANCE:\n",
                "   • R² Score: {metrics['Test']['R²']:.4f} ({metrics['Test']['R²']*100:.1f}% variance explained)\n",
                "   • MAE: ${metrics['Test']['MAE']:,.0f} average error\n",
                "   • RMSE: ${metrics['Test']['RMSE']:,.0f} typical error range\n",
                "\"\"\")\n",
                "\n",
                "print(\"=\" * 70)\n",
                "print(\" Ready for Day 7: Classification Algorithms!\")\n",
                "print(\"=\" * 70)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Practice Exercises\n",
                "\n",
                "Try these on your own:\n",
                "\n",
                "1. **Feature Engineering**: Create a new feature `Price_Per_SqFt` and see if it improves predictions\n",
                "2. **Different Split Ratios**: Try 70/30 and 90/10 splits and compare results\n",
                "3. **Polynomial Features**: Use `sklearn.preprocessing.PolynomialFeatures` to capture non-linear relationships\n",
                "4. **Cross-Validation**: Use `sklearn.model_selection.cross_val_score` for more robust evaluation\n",
                "\n",
                "---\n",
                "\n",
                "## Key Takeaways\n",
                "\n",
                "| Concept | Key Points |\n",
                "|---------|------------|\n",
                "| **Linear Regression** | Predicts continuous values using linear equation |\n",
                "| **Train/Test Split** | Essential to prevent overfitting |\n",
                "| **Coefficients** | Show feature importance and direction |\n",
                "| **R² Score** | 0-1 scale, higher means better fit |\n",
                "| **RMSE/MAE** | Lower is better, in same units as target |\n",
                "\n",
                "### When to Use Linear Regression:\n",
                "- Target variable is continuous\n",
                "- Relationship between features and target is approximately linear\n",
                "- You need interpretable results (coefficients explain relationships)\n",
                "\n",
                "---\n",
                "\n",
                "**Next Up:** Day 7 - Classification with Logistic Regression, Decision Trees & Random Forests!"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}