{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Day 10: Capstone Project - Wine Quality Prediction\n",
                "\n",
                "\n",
                "### Project Overview\n",
                "We'll build a complete ML pipeline to predict wine quality based on chemical properties.\n",
                "\n",
                "### The ML Pipeline:\n",
                "1. Data Loading & Understanding\n",
                "2. Exploratory Data Analysis (EDA)\n",
                "3. Data Preprocessing\n",
                "4. Model Training & Comparison\n",
                "5. Hyperparameter Tuning\n",
                "6. Final Evaluation & Insights"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import all libraries\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
                "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.tree import DecisionTreeClassifier\n",
                "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
                "from sklearn.svm import SVC\n",
                "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
                "                             confusion_matrix, classification_report)\n",
                "\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "np.random.seed(42)\n",
                "\n",
                "print(\" CAPSTONE PROJECT: WINE QUALITY PREDICTION\")\n",
                "print(\"=\"*50)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Step 1: Data Loading & Understanding"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create Wine Quality Dataset\n",
                "np.random.seed(42)\n",
                "n = 1500\n",
                "\n",
                "# Generate features based on wine chemistry\n",
                "fixed_acidity = np.random.normal(8.0, 1.5, n).clip(4, 15)\n",
                "volatile_acidity = np.random.normal(0.5, 0.2, n).clip(0.1, 1.5)\n",
                "citric_acid = np.random.normal(0.3, 0.15, n).clip(0, 1)\n",
                "residual_sugar = np.random.exponential(2.5, n).clip(0.5, 15)\n",
                "chlorides = np.random.normal(0.08, 0.03, n).clip(0.01, 0.3)\n",
                "free_sulfur = np.random.normal(15, 10, n).clip(1, 70)\n",
                "total_sulfur = free_sulfur + np.random.normal(30, 20, n).clip(5, 200)\n",
                "density = np.random.normal(0.996, 0.002, n).clip(0.99, 1.01)\n",
                "pH = np.random.normal(3.3, 0.2, n).clip(2.8, 4.0)\n",
                "sulphates = np.random.normal(0.6, 0.2, n).clip(0.2, 1.5)\n",
                "alcohol = np.random.normal(10.5, 1.5, n).clip(8, 15)\n",
                "\n",
                "# Quality based on features (3-9 scale, simplified to 3 classes)\n",
                "quality_score = (\n",
                "    -volatile_acidity * 2 +\n",
                "    citric_acid * 1.5 +\n",
                "    alcohol * 0.3 +\n",
                "    sulphates * 1 -\n",
                "    chlorides * 5 +\n",
                "    np.random.normal(0, 0.5, n)\n",
                ")\n",
                "quality = pd.cut(quality_score, bins=3, labels=['Low', 'Medium', 'High'])\n",
                "\n",
                "wine = pd.DataFrame({\n",
                "    'fixed_acidity': fixed_acidity.round(2),\n",
                "    'volatile_acidity': volatile_acidity.round(3),\n",
                "    'citric_acid': citric_acid.round(3),\n",
                "    'residual_sugar': residual_sugar.round(2),\n",
                "    'chlorides': chlorides.round(4),\n",
                "    'free_sulfur_dioxide': free_sulfur.round(1),\n",
                "    'total_sulfur_dioxide': total_sulfur.round(1),\n",
                "    'density': density.round(5),\n",
                "    'pH': pH.round(2),\n",
                "    'sulphates': sulphates.round(3),\n",
                "    'alcohol': alcohol.round(2),\n",
                "    'quality': quality\n",
                "})\n",
                "\n",
                "print(f\"Dataset Shape: {wine.shape}\")\n",
                "print(f\"\\nFirst 5 rows:\")\n",
                "wine.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Basic Statistics\n",
                "print(\" DATASET OVERVIEW\")\n",
                "print(\"=\"*50)\n",
                "print(f\"\\nSamples: {len(wine)}\")\n",
                "print(f\"Features: {len(wine.columns) - 1}\")\n",
                "print(f\"\\nTarget Distribution:\")\n",
                "print(wine['quality'].value_counts())\n",
                "print(f\"\\nMissing Values: {wine.isnull().sum().sum()}\")\n",
                "print(f\"\\nData Types:\\n{wine.dtypes}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Statistical Summary\n",
                "wine.describe().round(3)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Step 2: Exploratory Data Analysis (EDA)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Target Distribution\n",
                "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
                "\n",
                "# Count plot\n",
                "wine['quality'].value_counts().plot(kind='bar', ax=axes[0], color=['#e74c3c', '#f39c12', '#2ecc71'])\n",
                "axes[0].set_title('Wine Quality Distribution', fontweight='bold')\n",
                "axes[0].set_xlabel('Quality')\n",
                "axes[0].set_ylabel('Count')\n",
                "axes[0].tick_params(axis='x', rotation=0)\n",
                "\n",
                "# Pie chart\n",
                "wine['quality'].value_counts().plot(kind='pie', ax=axes[1], autopct='%1.1f%%',\n",
                "                                     colors=['#e74c3c', '#f39c12', '#2ecc71'])\n",
                "axes[1].set_title('Quality Percentage', fontweight='bold')\n",
                "axes[1].set_ylabel('')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Feature Distributions\n",
                "fig, axes = plt.subplots(3, 4, figsize=(16, 10))\n",
                "features = wine.columns[:-1]\n",
                "\n",
                "for i, (ax, feature) in enumerate(zip(axes.flatten(), features)):\n",
                "    wine[feature].hist(ax=ax, bins=30, color='#3498db', edgecolor='white', alpha=0.7)\n",
                "    ax.set_title(feature, fontsize=10)\n",
                "    ax.set_xlabel('')\n",
                "\n",
                "# Hide empty subplot\n",
                "axes[2, 3].set_visible(False)\n",
                "\n",
                "plt.suptitle('Feature Distributions', fontsize=14, fontweight='bold', y=1.02)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Correlation Heatmap\n",
                "plt.figure(figsize=(12, 8))\n",
                "\n",
                "# Encode quality for correlation\n",
                "wine_corr = wine.copy()\n",
                "wine_corr['quality_encoded'] = LabelEncoder().fit_transform(wine['quality'])\n",
                "corr = wine_corr.drop('quality', axis=1).corr()\n",
                "\n",
                "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
                "sns.heatmap(corr, mask=mask, annot=True, fmt='.2f', cmap='RdYlBu_r', \n",
                "            center=0, square=True, linewidths=0.5)\n",
                "plt.title('Feature Correlation Matrix', fontsize=14, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Key Features by Quality\n",
                "fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
                "key_features = ['alcohol', 'volatile_acidity', 'sulphates', 'citric_acid', 'chlorides', 'pH']\n",
                "\n",
                "for ax, feature in zip(axes.flatten(), key_features):\n",
                "    wine.boxplot(column=feature, by='quality', ax=ax)\n",
                "    ax.set_title(feature, fontweight='bold')\n",
                "    ax.set_xlabel('Quality')\n",
                "\n",
                "plt.suptitle('Key Features by Quality Level', fontsize=14, fontweight='bold', y=1.02)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Step 3: Data Preprocessing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare features and target\n",
                "X = wine.drop('quality', axis=1)\n",
                "y = LabelEncoder().fit_transform(wine['quality'])  # Low=0, Medium=2, High=1\n",
                "\n",
                "print(f\"Features shape: {X.shape}\")\n",
                "print(f\"Target classes: {np.unique(y)}\")\n",
                "print(f\"Class distribution: {np.bincount(y)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train-Test Split\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, test_size=0.2, random_state=42, stratify=y\n",
                ")\n",
                "\n",
                "print(f\"Training set: {len(X_train)} samples\")\n",
                "print(f\"Test set: {len(X_test)} samples\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Feature Scaling\n",
                "scaler = StandardScaler()\n",
                "X_train_scaled = scaler.fit_transform(X_train)\n",
                "X_test_scaled = scaler.transform(X_test)\n",
                "\n",
                "print(\"Features scaled!\")\n",
                "print(f\"Mean (before): {X_train.mean().mean():.3f}\")\n",
                "print(f\"Mean (after): {X_train_scaled.mean():.6f}\")\n",
                "print(f\"Std (after): {X_train_scaled.std():.3f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Step 4: Model Training & Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define models\n",
                "models = {\n",
                "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
                "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
                "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
                "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
                "    'SVM': SVC(random_state=42)\n",
                "}\n",
                "\n",
                "print(\" MODEL TRAINING & CROSS-VALIDATION\")\n",
                "print(\"=\"*55)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train and evaluate all models\n",
                "results = []\n",
                "\n",
                "for name, model in models.items():\n",
                "    # Use scaled data for distance-based models\n",
                "    if name in ['Logistic Regression', 'SVM']:\n",
                "        X_tr, X_te = X_train_scaled, X_test_scaled\n",
                "    else:\n",
                "        X_tr, X_te = X_train, X_test\n",
                "    \n",
                "    # Cross-validation\n",
                "    cv_scores = cross_val_score(model, X_tr, y_train, cv=5, scoring='accuracy')\n",
                "    \n",
                "    # Train and predict\n",
                "    model.fit(X_tr, y_train)\n",
                "    y_pred = model.predict(X_te)\n",
                "    \n",
                "    # Metrics\n",
                "    acc = accuracy_score(y_test, y_pred)\n",
                "    prec = precision_score(y_test, y_pred, average='weighted')\n",
                "    rec = recall_score(y_test, y_pred, average='weighted')\n",
                "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
                "    \n",
                "    results.append({\n",
                "        'Model': name,\n",
                "        'CV_Mean': cv_scores.mean(),\n",
                "        'CV_Std': cv_scores.std(),\n",
                "        'Test_Accuracy': acc,\n",
                "        'Precision': prec,\n",
                "        'Recall': rec,\n",
                "        'F1': f1\n",
                "    })\n",
                "    \n",
                "    print(f\"\\n{name}:\")\n",
                "    print(f\"  CV Score: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})\")\n",
                "    print(f\"  Test Accuracy: {acc:.4f}\")\n",
                "\n",
                "results_df = pd.DataFrame(results).sort_values('Test_Accuracy', ascending=False)\n",
                "print(\"\\n\" + \"=\"*55)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Results Comparison\n",
                "print(\" MODEL COMPARISON\")\n",
                "print(results_df[['Model', 'CV_Mean', 'Test_Accuracy', 'F1']].to_string(index=False))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize results\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Accuracy comparison\n",
                "colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(results_df)))\n",
                "bars = axes[0].barh(results_df['Model'], results_df['Test_Accuracy'], color=colors)\n",
                "axes[0].set_xlabel('Accuracy')\n",
                "axes[0].set_title('Model Accuracy Comparison', fontweight='bold')\n",
                "axes[0].set_xlim(0.5, 1.0)\n",
                "for bar, acc in zip(bars, results_df['Test_Accuracy']):\n",
                "    axes[0].text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2,\n",
                "                 f'{acc:.3f}', va='center')\n",
                "\n",
                "# All metrics\n",
                "metrics_df = results_df.set_index('Model')[['Test_Accuracy', 'Precision', 'Recall', 'F1']]\n",
                "metrics_df.plot(kind='bar', ax=axes[1], width=0.8)\n",
                "axes[1].set_title('All Metrics by Model', fontweight='bold')\n",
                "axes[1].set_ylabel('Score')\n",
                "axes[1].set_ylim(0.5, 1.0)\n",
                "axes[1].legend(loc='lower right')\n",
                "axes[1].tick_params(axis='x', rotation=45)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Step 5: Hyperparameter Tuning (Best Model)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Tune Random Forest (typically best performer)\n",
                "print(\" HYPERPARAMETER TUNING: Random Forest\")\n",
                "print(\"=\"*50)\n",
                "\n",
                "param_grid = {\n",
                "    'n_estimators': [50, 100, 200],\n",
                "    'max_depth': [5, 10, 15, 20],\n",
                "    'min_samples_split': [2, 5, 10],\n",
                "    'min_samples_leaf': [1, 2, 4]\n",
                "}\n",
                "\n",
                "grid_search = GridSearchCV(\n",
                "    RandomForestClassifier(random_state=42),\n",
                "    param_grid, cv=5, scoring='f1_weighted', n_jobs=-1, verbose=1\n",
                ")\n",
                "\n",
                "grid_search.fit(X_train, y_train)\n",
                "\n",
                "print(f\"\\nBest Parameters: {grid_search.best_params_}\")\n",
                "print(f\"Best CV Score: {grid_search.best_score_:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compare tuned vs default\n",
                "default_rf = models['Random Forest']\n",
                "tuned_rf = grid_search.best_estimator_\n",
                "\n",
                "y_pred_default = default_rf.predict(X_test)\n",
                "y_pred_tuned = tuned_rf.predict(X_test)\n",
                "\n",
                "print(\" DEFAULT vs TUNED RANDOM FOREST\")\n",
                "print(\"=\"*50)\n",
                "print(f\"\\n{'Metric':<15} {'Default':>12} {'Tuned':>12} {'Improvement':>12}\")\n",
                "print(\"-\"*52)\n",
                "\n",
                "for metric_name, metric_fn in [('Accuracy', accuracy_score), \n",
                "                                ('F1 Score', lambda y,p: f1_score(y,p,average='weighted'))]:\n",
                "    default_score = metric_fn(y_test, y_pred_default)\n",
                "    tuned_score = metric_fn(y_test, y_pred_tuned)\n",
                "    improvement = tuned_score - default_score\n",
                "    print(f\"{metric_name:<15} {default_score:>12.4f} {tuned_score:>12.4f} {improvement:>+12.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Step 6: Final Evaluation & Insights"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Final model evaluation\n",
                "best_model = tuned_rf\n",
                "y_pred_final = best_model.predict(X_test)\n",
                "\n",
                "# Confusion Matrix\n",
                "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
                "\n",
                "cm = confusion_matrix(y_test, y_pred_final)\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
                "            xticklabels=['High', 'Low', 'Medium'], yticklabels=['High', 'Low', 'Medium'])\n",
                "axes[0].set_title('Confusion Matrix (Tuned RF)', fontweight='bold')\n",
                "axes[0].set_xlabel('Predicted')\n",
                "axes[0].set_ylabel('Actual')\n",
                "\n",
                "# Feature Importance\n",
                "importance = pd.DataFrame({\n",
                "    'Feature': X.columns,\n",
                "    'Importance': best_model.feature_importances_\n",
                "}).sort_values('Importance', ascending=True)\n",
                "\n",
                "colors = plt.cm.RdYlGn(np.linspace(0.2, 0.8, len(importance)))\n",
                "axes[1].barh(importance['Feature'], importance['Importance'], color=colors)\n",
                "axes[1].set_title('Feature Importance', fontweight='bold')\n",
                "axes[1].set_xlabel('Importance')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Classification Report\n",
                "print(\" FINAL CLASSIFICATION REPORT\")\n",
                "print(\"=\"*50)\n",
                "print(classification_report(y_test, y_pred_final, target_names=['High', 'Low', 'Medium']))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Business Insights\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\" KEY INSIGHTS & RECOMMENDATIONS\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "top_features = importance.tail(5)['Feature'].tolist()[::-1]\n",
                "\n",
                "print(f\"\"\"\n",
                " MODEL PERFORMANCE:\n",
                "   Best Model: Tuned Random Forest\n",
                "   Accuracy: {accuracy_score(y_test, y_pred_final):.1%}\n",
                "   F1 Score: {f1_score(y_test, y_pred_final, average='weighted'):.1%}\n",
                "\n",
                " TOP 5 QUALITY PREDICTORS:\n",
                "   1. {top_features[0]}\n",
                "   2. {top_features[1]}\n",
                "   3. {top_features[2]}\n",
                "   4. {top_features[3]}\n",
                "   5. {top_features[4]}\n",
                "\n",
                " BUSINESS RECOMMENDATIONS:\n",
                "   - Focus quality control on volatile acidity levels\n",
                "   - Higher alcohol content correlates with better quality\n",
                "   - Sulphate levels significantly impact wine quality\n",
                "   - Use this model for automated quality assessment\n",
                "\"\"\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Predict on new wine samples\n",
                "print(\" PREDICT NEW WINE SAMPLES\")\n",
                "print(\"=\"*50)\n",
                "\n",
                "new_wines = pd.DataFrame({\n",
                "    'fixed_acidity': [7.5, 8.5, 6.8],\n",
                "    'volatile_acidity': [0.3, 0.7, 0.4],\n",
                "    'citric_acid': [0.4, 0.2, 0.35],\n",
                "    'residual_sugar': [2.0, 3.5, 1.8],\n",
                "    'chlorides': [0.07, 0.09, 0.065],\n",
                "    'free_sulfur_dioxide': [15, 12, 18],\n",
                "    'total_sulfur_dioxide': [45, 60, 40],\n",
                "    'density': [0.995, 0.997, 0.994],\n",
                "    'pH': [3.3, 3.4, 3.25],\n",
                "    'sulphates': [0.7, 0.5, 0.8],\n",
                "    'alcohol': [11.5, 9.5, 12.0]\n",
                "})\n",
                "\n",
                "predictions = best_model.predict(new_wines)\n",
                "quality_map = {0: 'High', 1: 'Low', 2: 'Medium'}\n",
                "\n",
                "for i, pred in enumerate(predictions):\n",
                "    quality = quality_map[pred]\n",
                "    print(f\"\\nWine {i+1}: {quality} Quality\")\n",
                "    print(f\"  Alcohol: {new_wines.iloc[i]['alcohol']}%\")\n",
                "    print(f\"  Volatile Acidity: {new_wines.iloc[i]['volatile_acidity']}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Final Summary\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\" CONGRATULATIONS! YOU'VE COMPLETED THE 10-DAY ML JOURNEY!\")\n",
                "print(\"=\"*70)\n",
                "print(\"\"\"\n",
                " WHAT YOU'VE LEARNED:\n",
                "\n",
                " Day 1-2: Python Fundamentals & Data Structures\n",
                " Day 3: NumPy - Numerical Computing\n",
                " Day 4: Pandas - Data Manipulation\n",
                " Day 5: Data Visualization (Matplotlib & Seaborn)\n",
                " Day 6: Linear Regression\n",
                " Day 7: Classification (Logistic, Trees, Random Forest)\n",
                " Day 8: Unsupervised Learning (K-Means, PCA)\n",
                " Day 9: Model Evaluation & Hyperparameter Tuning\n",
                " Day 10: Complete ML Pipeline (This Capstone!)\n",
                "\n",
                " YOUR ML TOOLKIT:\n",
                "   - Data Loading & Cleaning (Pandas)\n",
                "   - EDA & Visualization (Matplotlib, Seaborn)\n",
                "   - Preprocessing (Scaling, Encoding)\n",
                "   - Model Training (Scikit-Learn)\n",
                "   - Evaluation (Metrics, Cross-Validation)\n",
                "   - Optimization (GridSearchCV)\n",
                "\n",
                " NEXT STEPS:\n",
                "   1. Practice with Kaggle competitions\n",
                "   2. Learn Deep Learning (TensorFlow/PyTorch)\n",
                "   3. Explore NLP and Computer Vision\n",
                "   4. Build real-world projects\n",
                "   5. Never stop learning!\n",
                "\"\"\")\n",
                "print(\"=\"*70)\n",
                "print(\" Happy Machine Learning! \")\n",
                "print(\"=\"*70)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.13.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
